{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"exam_1_code_answer.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"2irv6MwjLZ5O","colab_type":"text"},"source":["# **Import File**"]},{"cell_type":"code","metadata":{"id":"ywWiGY-4l55p","colab_type":"code","outputId":"13df8cd0-8468-4be8-8b1c-999fee5c1187","executionInfo":{"status":"ok","timestamp":1590068476073,"user_tz":-420,"elapsed":981,"user":{"displayName":"supasun kumpraphan","photoUrl":"","userId":"09405620092166473782"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rAacho2wLf8U","colab_type":"text"},"source":["# **Import Library**"]},{"cell_type":"code","metadata":{"id":"Bceu1pcolvzW","colab_type":"code","colab":{}},"source":["import os \n","import glob\n","import numpy as np\n","import pandas as pd\n","\n","import numpy as np\n","import pandas as pd\n","from collections import defaultdict\n","from scipy.stats import hmean\n","from scipy.spatial.distance import cdist\n","from scipy import stats\n","import numbers\n","os.chdir(r\"/content/drive/My Drive/work/test-exam-data/code/exam_1\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ORTzUTRoLplI","colab_type":"text"},"source":["# **Survey Data**"]},{"cell_type":"code","metadata":{"id":"MMJd2Jvvlvza","colab_type":"code","colab":{}},"source":["data=pd.read_csv(\"DrugAllergyKaggle_v3_050720.csv\") ##Load Data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oSgdxXuqlvze","colab_type":"code","outputId":"760b9526-2557-4697-ba07-b027463db335","executionInfo":{"status":"ok","timestamp":1590068477047,"user_tz":-420,"elapsed":1903,"user":{"displayName":"supasun kumpraphan","photoUrl":"","userId":"09405620092166473782"}},"colab":{"base_uri":"https://localhost:8080/","height":439}},"source":["data"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Patient_ID</th>\n","      <th>Gender</th>\n","      <th>Age_Year</th>\n","      <th>ELISpot_Control</th>\n","      <th>ELISpot_Result</th>\n","      <th>Naranjo_Score</th>\n","      <th>Naranjo_Category</th>\n","      <th>Exposure_Time</th>\n","      <th>Steroid_Usage</th>\n","      <th>Underlying_Condition_A</th>\n","      <th>Underlying_Condition_B</th>\n","      <th>Underlying_Condition_C</th>\n","      <th>Underlying_Condition_D</th>\n","      <th>Underlying_Condition_E</th>\n","      <th>Suspicion_Score</th>\n","      <th>Allergic_Reaction_Group</th>\n","      <th>Drug_Group</th>\n","      <th>Drug_Rechallenge_Result</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>451</td>\n","      <td>0</td>\n","      <td>57</td>\n","      <td>2648</td>\n","      <td>554.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>8.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>2.0</td>\n","      <td>2</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>431</td>\n","      <td>0</td>\n","      <td>38</td>\n","      <td>2492</td>\n","      <td>542.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>10.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>7</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>378</td>\n","      <td>0</td>\n","      <td>58</td>\n","      <td>808</td>\n","      <td>439.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>4.0</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>6.0</td>\n","      <td>2</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>101</td>\n","      <td>0</td>\n","      <td>63</td>\n","      <td>2700</td>\n","      <td>431.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>16.0</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>2.0</td>\n","      <td>6</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>352</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>2060</td>\n","      <td>338.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3</td>\n","      <td>1.0</td>\n","      <td>3</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>794</th>\n","      <td>562</td>\n","      <td>0</td>\n","      <td>21</td>\n","      <td>700</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","      <td>2.0</td>\n","      <td>20.0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>2</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>795</th>\n","      <td>562</td>\n","      <td>0</td>\n","      <td>21</td>\n","      <td>700</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","      <td>2.0</td>\n","      <td>20.0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>5</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>796</th>\n","      <td>417</td>\n","      <td>0</td>\n","      <td>50</td>\n","      <td>2280</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>6.0</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>797</th>\n","      <td>417</td>\n","      <td>0</td>\n","      <td>50</td>\n","      <td>2280</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>6.0</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>6.0</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>798</th>\n","      <td>417</td>\n","      <td>0</td>\n","      <td>50</td>\n","      <td>2280</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>4</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>799 rows Ã— 18 columns</p>\n","</div>"],"text/plain":["     Patient_ID  Gender  ...  Drug_Group  Drug_Rechallenge_Result\n","0           451       0  ...           2                      NaN\n","1           431       0  ...           7                      NaN\n","2           378       0  ...           2                      NaN\n","3           101       0  ...           6                      NaN\n","4           352       0  ...           3                      NaN\n","..          ...     ...  ...         ...                      ...\n","794         562       0  ...           2                      NaN\n","795         562       0  ...           5                      1.0\n","796         417       0  ...           1                      NaN\n","797         417       0  ...           1                      0.0\n","798         417       0  ...           4                      NaN\n","\n","[799 rows x 18 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"A7juYoC8L0K7","colab_type":"text"},"source":["#we have 79 records which Naranjo Category is not null but Naranjo_Score is null we will impute it by mode imputation of \n","##that Category \n","#we compute it in excel since it very much easier it smaleer\n","##Category = 0 we impute Score = 0\n","##Category = 1 we impute Score = 3\n","##Category = 2 we impute Score = 5\n","##Category = 3 we impute Score = 8"]},{"cell_type":"code","metadata":{"id":"T9tD_qkclvzp","colab_type":"code","colab":{}},"source":["def weighted_hamming(data):\n","    \"\"\" Compute weighted hamming distance on categorical variables. For one variable, it is equal to 1 if\n","        the values between point A and point B are different, else it is equal the relative frequency of the\n","        distribution of the value across the variable. For multiple variables, the harmonic mean is computed\n","        up to a constant factor.\n","        @params:\n","            - data = a pandas data frame of categorical variables\n","        @returns:\n","            - distance_matrix = a distance matrix with pairwise distance for all attributes\n","    \"\"\"\n","    categories_dist = []\n","    \n","    for category in data:\n","        X = pd.get_dummies(data[category])\n","        X_mean = X * X.mean()\n","        X_dot = X_mean.dot(X.transpose())\n","        X_np = np.asarray(X_dot.replace(0,1,inplace=False))\n","        categories_dist.append(X_np)\n","    categories_dist = np.array(categories_dist)\n","    distances = hmean(categories_dist, axis=0)\n","    return distances\n","\n","\n","def distance_matrix(data, numeric_distance = \"euclidean\", categorical_distance = \"jaccard\"):\n","    \"\"\" Compute the pairwise distance attribute by attribute in order to account for different variables type:\n","        - Continuous\n","        - Categorical\n","        For ordinal values, provide a numerical representation taking the order into account.\n","        Categorical variables are transformed into a set of binary ones.\n","        If both continuous and categorical distance are provided, a Gower-like distance is computed and the numeric\n","        variables are all normalized in the process.\n","        If there are missing values, the mean is computed for numerical attributes and the mode for categorical ones.\n","        \n","        Note: If weighted-hamming distance is chosen, the computation time increases a lot since it is not coded in C \n","        like other distance metrics provided by scipy.\n","        @params:\n","            - data                  = pandas dataframe to compute distances on.\n","            - numeric_distances     = the metric to apply to continuous attributes.\n","                                      \"euclidean\" and \"cityblock\" available.\n","                                      Default = \"euclidean\"\n","            - categorical_distances = the metric to apply to binary attributes.\n","                                      \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n","                                      available. Default = \"jaccard\"\n","        @returns:\n","            - the distance matrix\n","    \"\"\"\n","    possible_continuous_distances = [\"euclidean\", \"cityblock\"]\n","    possible_binary_distances = [\"euclidean\", \"jaccard\", \"hamming\", \"weighted-hamming\"]\n","    number_of_variables = data.shape[1]\n","    number_of_observations = data.shape[0]\n","\n","    # Get the type of each attribute (Numeric or categorical)\n","    is_numeric = [all(isinstance(n, numbers.Number) for n in data.iloc[:, i]) for i, x in enumerate(data)]\n","    is_all_numeric = sum(is_numeric) == len(is_numeric)\n","    is_all_categorical = sum(is_numeric) == 0\n","    is_mixed_type = not is_all_categorical and not is_all_numeric\n","\n","    # Check the content of the distances parameter\n","    if numeric_distance not in possible_continuous_distances:\n","        print(\"The continuous distance \" + numeric_distance + \" is not supported.\")\n","        return None\n","    elif categorical_distance not in possible_binary_distances:\n","        print(\"The binary distance \" + categorical_distance + \" is not supported.\")\n","        return None\n","\n","    # Separate the data frame into categorical and numeric attributes and normalize numeric data\n","    if is_mixed_type:\n","        number_of_numeric_var = sum(is_numeric)\n","        number_of_categorical_var = number_of_variables - number_of_numeric_var\n","        data_numeric = data.iloc[:, is_numeric]\n","        data_numeric = (data_numeric - data_numeric.mean()) / (data_numeric.max() - data_numeric.min())\n","        data_categorical = data.iloc[:, [not x for x in is_numeric]]\n","\n","    # Replace missing values with column mean for numeric values and mode for categorical ones. With the mode, it\n","    # triggers a warning: \"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\"\n","    # but the value are properly replaced\n","    if is_mixed_type:\n","        data_numeric.fillna(data_numeric.mean(), inplace=True)\n","        for x in data_categorical:\n","            data_categorical[x].fillna(data_categorical[x].mode()[0], inplace=True)\n","    elif is_all_numeric:\n","        data.fillna(data.mean(), inplace=True)\n","    else:\n","        for x in data:\n","            data[x].fillna(data[x].mode()[0], inplace=True)\n","\n","    # \"Dummifies\" categorical variables in place\n","    if not is_all_numeric and not (categorical_distance == 'hamming' or categorical_distance == 'weighted-hamming'):\n","        if is_mixed_type:\n","            data_categorical = pd.get_dummies(data_categorical)\n","        else:\n","            data = pd.get_dummies(data)\n","    elif not is_all_numeric and categorical_distance == 'hamming':\n","        if is_mixed_type:\n","            data_categorical = pd.DataFrame([pd.factorize(data_categorical[x])[0] for x in data_categorical]).transpose()\n","        else:\n","            data = pd.DataFrame([pd.factorize(data[x])[0] for x in data]).transpose()\n","\n","    if is_all_numeric:\n","        result_matrix = cdist(data, data, metric=numeric_distance)\n","    elif is_all_categorical:\n","        if categorical_distance == \"weighted-hamming\":\n","            result_matrix = weighted_hamming(data)\n","        else:\n","            result_matrix = cdist(data, data, metric=categorical_distance)\n","    else:\n","        result_numeric = cdist(data_numeric, data_numeric, metric=numeric_distance)\n","        if categorical_distance == \"weighted-hamming\":\n","            result_categorical = weighted_hamming(data_categorical)\n","        else:\n","            result_categorical = cdist(data_categorical, data_categorical, metric=categorical_distance)\n","        result_matrix = np.array([[1.0*(result_numeric[i, j] * number_of_numeric_var + result_categorical[i, j] *\n","                               number_of_categorical_var) / number_of_variables for j in range(number_of_observations)] for i in range(number_of_observations)])\n","\n","    # Fill the diagonal with NaN values\n","    np.fill_diagonal(result_matrix, np.nan)\n","\n","    return pd.DataFrame(result_matrix)\n","\n","\n","def knn_impute(target, attributes, k_neighbors, aggregation_method=\"mean\", numeric_distance=\"euclidean\",\n","               categorical_distance=\"jaccard\", missing_neighbors_threshold = 0.5):\n","    \"\"\" Replace the missing values within the target variable based on its k nearest neighbors identified with the\n","        attributes variables. If more than 50% of its neighbors are also missing values, the value is not modified and\n","        remains missing. If there is a problem in the parameters provided, returns None.\n","        If to many neighbors also have missing values, leave the missing value of interest unchanged.\n","        @params:\n","            - target                        = a vector of n values with missing values that you want to impute. The length has\n","                                              to be at least n = 3.\n","            - attributes                    = a data frame of attributes with n rows to match the target variable\n","            - k_neighbors                   = the number of neighbors to look at to impute the missing values. It has to be a\n","                                              value between 1 and n.\n","            - aggregation_method            = how to aggregate the values from the nearest neighbors (mean, median, mode)\n","                                              Default = \"mean\"\n","            - numeric_distances             = the metric to apply to continuous attributes.\n","                                              \"euclidean\" and \"cityblock\" available.\n","                                              Default = \"euclidean\"\n","            - categorical_distances         = the metric to apply to binary attributes.\n","                                              \"jaccard\", \"hamming\", \"weighted-hamming\" and \"euclidean\"\n","                                              available. Default = \"jaccard\"\n","            - missing_neighbors_threshold   = minimum of neighbors among the k ones that are not also missing to infer\n","                                              the correct value. Default = 0.5\n","        @returns:\n","            target_completed        = the vector of target values with missing value replaced. If there is a problem\n","                                      in the parameters, return None\n","    \"\"\"\n","\n","    # Get useful variables\n","    possible_aggregation_method = [\"mean\", \"median\", \"mode\"]\n","    number_observations = len(target)\n","    is_target_numeric = all(isinstance(n, numbers.Number) for n in target)\n","\n","    # Check for possible errors\n","    if number_observations < 3:\n","        print(\"Not enough observations.\")\n","        return None\n","    if attributes.shape[0] != number_observations:\n","        print(\"The number of observations in the attributes variable is not matching the target variable length.\")\n","        return None\n","    if k_neighbors > number_observations or k_neighbors < 1:\n","        print (\"The range of the number of neighbors is incorrect.\")\n","        return None\n","    if aggregation_method not in possible_aggregation_method:\n","        print (\"The aggregation method is incorrect.\")\n","        return None\n","    if not is_target_numeric and aggregation_method != \"mode\":\n","        print (\"The only method allowed for categorical target variable is the mode.\")\n","        return None\n","\n","    # Make sure the data are in the right format\n","    target = pd.DataFrame(target)\n","    attributes = pd.DataFrame(attributes)\n","\n","    # Get the distance matrix and check whether no error was triggered when computing it\n","    distances = distance_matrix(attributes, numeric_distance, categorical_distance)\n","    if distances is None:\n","        return None\n","\n","    # Get the closest points and compute the correct aggregation method\n","    for i, value in enumerate(target.iloc[:, 0]):\n","        if pd.isnull(value):\n","            order = distances.iloc[i,:].values.argsort()[:k_neighbors]\n","            closest_to_target = target.iloc[order, :]\n","            missing_neighbors = [x for x  in closest_to_target.isnull().iloc[:, 0]]\n","            # Compute the right aggregation method if at least more than 50% of the closest neighbors are not missing\n","            if sum(missing_neighbors) >= missing_neighbors_threshold * k_neighbors:\n","                continue\n","            elif aggregation_method == \"mean\":\n","                target.iloc[i] = np.ma.mean(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n","            elif aggregation_method == \"median\":\n","                target.iloc[i] = np.ma.median(np.ma.masked_array(closest_to_target,np.isnan(closest_to_target)))\n","            else:\n","                target.iloc[i] = stats.mode(closest_to_target, nan_policy='omit')[0][0]\n","\n","    return target"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2URyv1CDlvzt","colab_type":"code","colab":{}},"source":["#impute naranjo missing value with knn nearest neighbour but remove Patient ID since it's has nothing to do with distance of\n","##each patient\n","\n","naranjo_score_impute=knn_impute(target=data['Naranjo_Score'], attributes=data.drop(['Patient_ID'], 1),\n","                                    aggregation_method=\"median\", k_neighbors=10, numeric_distance='euclidean',\n","                                    categorical_distance='hamming', missing_neighbors_threshold=0.8)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BpNRnMHxlvzx","colab_type":"code","colab":{}},"source":["data['Naranjo_Score']=naranjo_score_impute['Naranjo_Score']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UzTrqr5slvz3","colab_type":"code","colab":{}},"source":["##we clean data by drop Patient_ID and Exposure Time\n","\n","data=data.drop(['Patient_ID','Exposure_Time'],axis=1)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2tssHLQTlvz7","colab_type":"code","colab":{}},"source":["##then we drop null value to create train dataset\n","data=data.dropna()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZlhSw1rqlvz-","colab_type":"code","outputId":"1fecd953-7083-44a8-8082-2e9f064d952d","executionInfo":{"status":"ok","timestamp":1590068477888,"user_tz":-420,"elapsed":2652,"user":{"displayName":"supasun kumpraphan","photoUrl":"","userId":"09405620092166473782"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["#start by define dependant variable with y and independant with x\n","# y= data.iloc[:,17]\n","y= data.iloc[:,15]\n","\n","x=data.drop('Drug_Rechallenge_Result',axis=1)\n","y"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16     0.0\n","19     1.0\n","27     1.0\n","31     0.0\n","37     1.0\n","      ... \n","763    0.0\n","777    0.0\n","790    0.0\n","795    1.0\n","797    0.0\n","Name: Drug_Rechallenge_Result, Length: 97, dtype: float64"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"foU8GNXclv0K","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=27) #split train and test set"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JLXGw_bSlv0R","colab_type":"code","outputId":"88d266b7-e2ab-4ab7-abd1-4a9964a8f962","executionInfo":{"status":"ok","timestamp":1590068479802,"user_tz":-420,"elapsed":4532,"user":{"displayName":"supasun kumpraphan","photoUrl":"","userId":"09405620092166473782"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["# Instantiate model with 100 decision trees\n","rf = RandomForestClassifier(n_estimators = 1000, random_state = 42,min_samples_leaf=1, min_samples_split=2,max_depth=8)\n","# Train the model on training data\n","rf.fit(X_train, y_train)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n","                       criterion='gini', max_depth=8, max_features='auto',\n","                       max_leaf_nodes=None, max_samples=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n","                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n","                       warm_start=False)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"h-fspejRlv0Z","colab_type":"code","colab":{}},"source":["predictions = rf.predict(X_train) ##test_predictions on X_train of random forest"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hbEkVn0olv0f","colab_type":"code","outputId":"981c3b4b-6579-4566-cda8-804998d841b7","executionInfo":{"status":"ok","timestamp":1590068479806,"user_tz":-420,"elapsed":4506,"user":{"displayName":"supasun kumpraphan","photoUrl":"","userId":"09405620092166473782"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["##random forest result \n","\n","from sklearn.metrics import classification_report\n","from sklearn import metrics\n","print(metrics.confusion_matrix(y_train, predictions, labels=[0,1]))\n","print(classification_report(y_train, predictions))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["[[53  0]\n"," [ 1 23]]\n","              precision    recall  f1-score   support\n","\n","         0.0       0.98      1.00      0.99        53\n","         1.0       1.00      0.96      0.98        24\n","\n","    accuracy                           0.99        77\n","   macro avg       0.99      0.98      0.98        77\n","weighted avg       0.99      0.99      0.99        77\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qn3HVn2ilv0t","colab_type":"code","colab":{}},"source":["predictions = rf.predict(X_test) ##test_predictions on X_test of random forest"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eN_daaX8lv0x","colab_type":"code","outputId":"abce5447-18e2-46bc-b526-01c8ec896e75","executionInfo":{"status":"ok","timestamp":1590068479809,"user_tz":-420,"elapsed":4476,"user":{"displayName":"supasun kumpraphan","photoUrl":"","userId":"09405620092166473782"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["##random forest result \n","\n","from sklearn.metrics import classification_report\n","from sklearn import metrics\n","print(metrics.confusion_matrix(y_test, predictions, labels=[0,1]))\n","print(classification_report(y_test, predictions))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["[[15  2]\n"," [ 2  1]]\n","              precision    recall  f1-score   support\n","\n","         0.0       0.88      0.88      0.88        17\n","         1.0       0.33      0.33      0.33         3\n","\n","    accuracy                           0.80        20\n","   macro avg       0.61      0.61      0.61        20\n","weighted avg       0.80      0.80      0.80        20\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mjjwVZuJoLJC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}